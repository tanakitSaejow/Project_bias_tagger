{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f2b790",
   "metadata": {},
   "source": [
    "## Reference \n",
    "   - https://python3.wannaphong.com/2018/12/named-entity-recognition-ner-pythainlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6beaa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "from pythainlp.tag import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import glob\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5c7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#จัดการประโยคซ้ำ\n",
    "def Unique(p):\n",
    "    text=re.sub(\"<^[>]*>\",\"\",p)\n",
    "    text=re.sub(\"\\[(.*?)\\]\",\"\",text)\n",
    "    text=re.sub(\"\\[\\/(.*?)\\]\",\"\",text)\n",
    "    if text not in data_not:\n",
    "        data_not.append(text)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# อ่านข้อมูลจากไฟล์\n",
    "def get_data(fileopen):\n",
    "    \"\"\"\n",
    "    สำหรับใช้อ่านทั้งหมดทั้งในไฟล์ทีละรรทัดออกมาเป็น list\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    str_line = \"\"\n",
    "    with open(fileopen, 'r',encoding='utf-8-sig') as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            if(line == \"\\n\"):\n",
    "                lines.append(str_line)\n",
    "                lines[-1] = lines[-1].replace(\"\\n\",\" \")\n",
    "                str_line = \"\"\n",
    "            else:\n",
    "                str_line += line\n",
    "        lines.append(str_line.replace(\"\\n\",\" \"))\n",
    "    return [a for a in lines if Unique(a)] # เอาไม่ซ้ำกัน\n",
    "\n",
    "def getall(lista):\n",
    "    ll=[]\n",
    "    for i in lista:\n",
    "        o=True\n",
    "        for j in ll:\n",
    "            if re.sub(\"\\[(.*?)\\]\",\"\",i)==re.sub(\"\\[(.*?)\\]\",\"\",j):\n",
    "                o=False\n",
    "                break\n",
    "        if o==True:\n",
    "            ll.append(i)\n",
    "    return ll\n",
    "    \n",
    "\n",
    "# ใช้สำหรับกำกับ pos tag เพื่อใช้กับ NER\n",
    "# print(text2conll2002(t,pos=False))\n",
    "def postag(text):\n",
    "    listtxt=[i for i in text.split('\\n') if i!='']\n",
    "    list_word=[]\n",
    "    for data in listtxt:\n",
    "        list_word.append(data.split('\\t')[0])\n",
    "    #print(text)\n",
    "    list_word=pos_tag(list_word,engine=\"perceptron\")\n",
    "    text=\"\"\n",
    "    i=0\n",
    "    for data in listtxt:\n",
    "        text+=data.split('\\t')[0]+'\\t'+list_word[i][1]+'\\t'+data.split('\\t')[1]+'\\n'  #เราจะไปเดินเล่นที่\\tO\\nหนองคาย\\tB-location\\n\n",
    "        i+=1\n",
    "    return text\n",
    "\n",
    "\n",
    "# จัดการกับ tag ที่ไม่ได้ tag\n",
    "def toolner_to_tag(text):\n",
    "    text=text.strip()\n",
    "    text=re.sub(\"<[^>]*>\",\"\",text)\n",
    "    text=re.sub(\"(\\[\\/(.*?)\\])\",\"\\\\1***\",text)#.replace('(\\[(.*?)\\])','***\\\\1')#  ตัดการกับพวกไม่มี tag word\n",
    "    text=re.sub(\"(\\[\\w+\\])\",\"***\\\\1\",text)\n",
    "    text2=[]\n",
    "    for i in text.split('***'):\n",
    "        if \"[\" in i:\n",
    "            text2.append(i)\n",
    "        else:\n",
    "            text2.append(\"[word]\"+i+\"[/word]\")\n",
    "    text=\"\".join(text2)#re.sub(\"[word][/word]\",\"\",\"\".join(text2))\n",
    "    return text.replace(\"[word][/word]\",\"\")\n",
    "\n",
    "# เตรียมตัวตัด tag ด้วย re\n",
    "pattern = r'\\[(.*?)\\](.*?)\\[\\/(.*?)\\]'\n",
    "tokenizer = RegexpTokenizer(pattern) # ใช้ nltk.tokenize.RegexpTokenizer เพื่อตัด [TIME]8.00[/TIME] ให้เป็น ('TIME','ไง','TIME')\n",
    "\n",
    "# แปลง text ให้เป็น conll2002\n",
    "def text2conll2002(text,pos=True):\n",
    "    \"\"\"\n",
    "    ใช้แปลงข้อความให้กลายเป็น conll2002\n",
    "    \"\"\"\n",
    "    text=toolner_to_tag(text) # นำไปใส่ tag [word]\n",
    "    text=text.replace(\"''\",'\"')\n",
    "    text=text.replace(\"’\",'\"').replace(\"‘\",'\"')#.replace('\"',\"\")\n",
    "    tag=tokenizer.tokenize(text) # แยก tag ออกมาจากข้อความ\n",
    "    j=0\n",
    "    conll2002=\"\" # ประกาศตัวแปรเก็บ conll2002\n",
    "    for tagopen,text,tagclose in tag: # ลูปใน tag โดยเป็น (tagopen,text,tagclose)\n",
    "        word_cut=word_tokenize(text,engine=\"newmm\") # ใช้ตัวตัดคำ newmm ของ PyThaiNLP\n",
    "        i=0\n",
    "        txt5=\"\"\n",
    "        while i<len(word_cut): #ลูปตามจำนวน token ที่ตัดในtag\n",
    "            if word_cut[i]==\"''\" or word_cut[i]=='\"':pass\n",
    "            elif i==0 and tagopen!='word': # ไม่เป็น tag [word] และเป็น i หรือตัวเริ่มต้น tag\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'B-'+tagopen\n",
    "            elif tagopen!='word':\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'I-'+tagopen \n",
    "            else: # เป็น [word]\n",
    "                txt5+=word_cut[i]\n",
    "                txt5+='\\t'+'O'\n",
    "            txt5+='\\n' #ลาว\\t-location\\n\n",
    "            #j+=1\n",
    "            i+=1\n",
    "        conll2002+=txt5\n",
    "    if pos==False:\n",
    "        return conll2002\n",
    "    return postag(conll2002) # เพิ่ม postag ใส่\n",
    "\n",
    "def alldata_list(lists, pos = True):\n",
    "    data_all=[]\n",
    "    for data in lists:        \n",
    "        data_num=[]\n",
    "        try:\n",
    "            txt=text2conll2002(data,pos=pos).split('\\n') # นำไปแปลงเป็น conll2002\n",
    "            for d in txt:\n",
    "                tt=d.split('\\t')\n",
    "                if d!=\"\":\n",
    "                    if len(tt)==3:\n",
    "                        data_num.append((tt[0],tt[1],tt[2])) \n",
    "                    else:\n",
    "                        data_num.append((tt[0],tt[1]))\n",
    "            #print(data_num)\n",
    "            data_all.append(data_num)\n",
    "        except:\n",
    "            print(data)\n",
    "#     print(data_all)\n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fc72f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ใน', 'RPRE', 'O'),\n",
       " ('บันทึก', 'NCMN', 'O'),\n",
       " ('ย่อ', 'VATT', 'B-d'),\n",
       " ('ที่', 'PREL', 'O'),\n",
       " ('น่าเศร้า', 'VSTA', 'B-e'),\n",
       " ('เมื่อ', 'JSBR', 'O'),\n",
       " ('วันที่', 'NCMN', 'O'),\n",
       " (' ', 'PUNC', 'O'),\n",
       " ('15', 'DCNM', 'O'),\n",
       " (' ', 'PUNC', 'O'),\n",
       " ('กันยายน', 'NCMN', 'O'),\n",
       " (' ', 'PUNC', 'O'),\n",
       " ('2547', 'NCNM', 'O'),\n",
       " (' ', 'PUNC', 'O'),\n",
       " ('จอห์น', 'VACT', 'O'),\n",
       " ('นี่', 'NCMN', 'O'),\n",
       " ('รา', 'NCMN', 'B-e'),\n",
       " ('โมน', 'NCMN', 'I-e'),\n",
       " (' ', 'PUNC', 'O'),\n",
       " ('(', 'PUNC', 'O'),\n",
       " ('ซึ่ง', 'JSBR', 'B-e'),\n",
       " ('แสดง', 'VACT', 'I-e'),\n",
       " ('ใน', 'RPRE', 'O'),\n",
       " ('ตอนนี้', 'NCMN', 'O'),\n",
       " ('กับ', 'RPRE', 'O'),\n",
       " ('รา', 'NCMN', 'O'),\n",
       " ('โมน', 'NCMN', 'O'),\n",
       " ('ส์)', 'NCMN', 'O'),\n",
       " (' ', 'PUNC', 'O'),\n",
       " ('เสียชีวิต', 'NCMN', 'O')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../dataset/tagged_data/\"\n",
    "data_not=[]\n",
    "data1  = []\n",
    "\n",
    "for name_folder in os.listdir(path):\n",
    "    for f_name in os.listdir(path+name_folder):\n",
    "        data1 += getall(get_data(path+name_folder+\"/\"+f_name))\n",
    "        \n",
    "datatofile_ = alldata_list(data1, pos=False)\n",
    "datatofile_pos = alldata_list(data1, pos=True) # นำไปผ่านขั้นตอน 1 2 3 4\n",
    "datatofile_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df93c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_empty_list(datatofile):\n",
    "    temp_data = []\n",
    "    for data in datatofile:\n",
    "        if data != []:\n",
    "            temp_data.append(data)\n",
    "    return temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6341fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datatofile_ = clear_empty_list(datatofile_)\n",
    "datatofile_pos = clear_empty_list(datatofile_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cd4098",
   "metadata": {},
   "source": [
    "# # save data for train (CRF, LSTM, BiLSTM, BiLSTM-CRF, Bert(transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d27956a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../dataset/CoNLL2002-dataset/\"\n",
    "file_name=\"biased\"\n",
    "\n",
    "with open(path+file_name+\"-pos.conll\",\"w\", encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile_pos):\n",
    "        for j in datatofile_pos[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\t\"+j[2]+\"\\n\")\n",
    "        if i+1<len(datatofile_pos):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1\n",
    "\n",
    "with open(path+file_name+\".conll\",\"w\",  encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(datatofile_):\n",
    "        for j in datatofile_[i]:\n",
    "            f.write(j[0]+\"\\t\"+j[1]+\"\\n\")\n",
    "        if i+1<len(datatofile_):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af31b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999\n",
      "[('ใน', 'RPRE', 'O'), ('บันทึก', 'NCMN', 'O'), ('ย่อ', 'VATT', 'B-d'), ('ที่', 'PREL', 'O'), ('น่าเศร้า', 'VSTA', 'B-e'), ('เมื่อ', 'JSBR', 'O'), ('วันที่', 'NCMN', 'O'), (' ', 'PUNC', 'O'), ('15', 'DCNM', 'O'), (' ', 'PUNC', 'O'), ('กันยายน', 'NCMN', 'O'), (' ', 'PUNC', 'O'), ('2547', 'NCNM', 'O'), (' ', 'PUNC', 'O'), ('จอห์น', 'VACT', 'O'), ('นี่', 'NCMN', 'O'), ('รา', 'NCMN', 'B-e'), ('โมน', 'NCMN', 'I-e'), (' ', 'PUNC', 'O'), ('(', 'PUNC', 'O'), ('ซึ่ง', 'JSBR', 'B-e'), ('แสดง', 'VACT', 'I-e'), ('ใน', 'RPRE', 'O'), ('ตอนนี้', 'NCMN', 'O'), ('กับ', 'RPRE', 'O'), ('รา', 'NCMN', 'O'), ('โมน', 'NCMN', 'O'), ('ส์)', 'NCMN', 'O'), (' ', 'PUNC', 'O'), ('เสียชีวิต', 'NCMN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "path = \"../dataset/data/\"\n",
    "\n",
    "pickle.dump(datatofile_pos, open(path+\"biased-pos.data\", 'wb'))\n",
    "pickle.dump(datatofile_, open(path+\"biased.data\", 'wb'))\n",
    "print(len(datatofile_pos))\n",
    "print(datatofile_pos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e82bb",
   "metadata": {},
   "source": [
    "# save data for train bert lazy(simpletranformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a470aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tagged_sents = []\n",
    "for data in datatofile_:\n",
    "    text_inside = []\n",
    "    for word, label in data:\n",
    "        if word.strip() == '':\n",
    "            text_inside.append(('_', label))\n",
    "        else:\n",
    "            text_inside.append((word, label))\n",
    "    tagged_sents.append(text_inside)\n",
    "\n",
    "train_sents, test_sents = train_test_split(tagged_sents, test_size=0.2, random_state=42)\n",
    "\n",
    "print(len(train_sents))\n",
    "print(len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42e6cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_format_label(datatofile):\n",
    "    data_file_bert = []\n",
    "    for data in datatofile:\n",
    "        list_word_lable = []\n",
    "        for word_label in data:\n",
    "            word = word_label[0]\n",
    "            label = word_label[1]\n",
    "            label = label.upper().replace(\"-\", \"_\")\n",
    "            list_word_lable.append((word, label))\n",
    "        data_file_bert.append(list_word_lable)\n",
    "    return data_file_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bd3a9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Michael', 'O'), ('_', 'O'), ('McDowell', 'O'), ('_', 'O'), ('เกิด', 'O'), ('ที่', 'O'), ('ดับ', 'O'), ('ลิ', 'O'), ('น', 'O'), ('ประเทศ', 'O'), ('ไอร์แลนด์', 'O'), ('และ', 'O'), ('ได้รับ', 'O'), ('การศึกษา', 'O'), ('ที่', 'O'), ('Elite', 'B_E'), ('_', 'I_E'), ('Private', 'I_E'), ('_', 'I_E'), ('Jesuit', 'I_E'), ('_', 'I_E'), ('School', 'I_E'), ('_', 'O'), ('Gonzaga', 'O'), ('_', 'O'), ('College', 'O'), ('_', 'O'), ('และ', 'O'), ('จากนั้น', 'O'), ('ที่', 'O'), ('_', 'O'), ('University', 'O'), ('_', 'O'), ('College', 'O'), ('_', 'O'), ('Dublin', 'O'), ('_', 'O'), ('และ', 'O'), ('_', 'O'), ('King', 'O'), (\"'\", 'O'), ('s', 'O'), ('_', 'O'), ('Inns', 'O'), ('_', 'O'), ('ใน', 'O'), ('ดับ', 'O'), ('ลิ', 'O'), ('น', 'O'), ('ซึ่ง', 'O'), ('เขา', 'O'), ('มี', 'O'), ('คุณสมบัติ', 'O'), ('เป็น', 'O'), ('ทนายความ', 'O')]\n"
     ]
    }
   ],
   "source": [
    "data_train = convert_format_label(train_sents)\n",
    "data_test = convert_format_label(test_sents)\n",
    "print(data_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a421adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "path = \"../dataset/CoNLL2002-dataset/\"\n",
    "file_name=\"train\"\n",
    "\n",
    "with open(path+file_name+\"_CONLL_BERT.txt\",\"w\",  encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(data_train):\n",
    "        for j in data_train[i]:\n",
    "            f.write(j[0]+\" \"+j[1]+\"\\n\")\n",
    "        if i+1<len(data_train):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f408fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "path = \"../dataset/CoNLL2002-dataset/\"\n",
    "file_name=\"test\"\n",
    "\n",
    "with open(path+file_name+\"_CONLL_BERT.txt\",\"w\",  encoding=\"utf-8-sig\") as f:\n",
    "    i=0\n",
    "    while i<len(data_test):\n",
    "        for j in data_test[i]:\n",
    "            f.write(j[0]+\" \"+j[1]+\"\\n\")\n",
    "        if i+1<len(data_test):\n",
    "            f.write(\"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245d8de",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b318c531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../dataset/data/\"\n",
    "\n",
    "with open(path+\"biased-pos.data\", 'rb') as file:\n",
    "    datatofile = dill.load(file)\n",
    "\n",
    "len(datatofile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129156b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
